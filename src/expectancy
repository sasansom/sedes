#!/usr/bin/env python3

# Computes an expectancy score for each unique lemma/sedes pair in the input
# files, as produced by tei2csv.
#
# Usage:
#   expectancy corpus/*.csv > expectancy.csv
#
# Writes CSV to standard output. Besides lemma and sedes, there are two columns:
#   x: The number of times this lemma/sedes pair appears
#   z: The expectancy score, weighted as if each x value were repeated x times.

import csv
import getopt
import math
import sys

def usage(file=sys.stdout):
    print(f"""\
Usage: {sys.argv[0]} [INPUT.csv...] > OUTPUT.csv

Computes expectancy z-scores for every unique lemma/sedes pair found in
the corpus of INPUT.csv files. The INPUT.csv files are the output of tei2csv.
""", end="", file=file)

# Return the mean of the sequence that arises from repeating each element e of
# x, e times.
def weighted_mean(x):
    return sum(e*e for e in x) / sum(x)

# Weighted population standard deviation, as in weighted_mean.
def weighted_sd_pop(x):
    μ = weighted_mean(x)
    return math.sqrt(sum(e*(e - μ)**2 for e in x) / sum(x))

# We special-case certain column names to transform their contained values on
# input.
INPUT_TRANSFORMS = {
    # Parse numeric sedes if present, otherwise set to None.
    "sedes": lambda val, row: float(val) if val else None,
}

def read_input(f):
    rows = []
    r = csv.DictReader(f)
    for row in r:
        for k in row:
            f = INPUT_TRANSFORMS.get(k)
            if f is not None:
                row[k] = f(row[k], row)
        rows.append(row)
    return rows

# Yields contiguous slices of x that are equal in fieldnames, along with a tuple
# containing the values that fieldnames maps to for the group.
def group_by(x, *fieldnames):
    i = 0
    j = 0
    prev_key = None
    for row in x:
        key = tuple(row[fieldname] for fieldname in fieldnames)
        if prev_key is not None and key != prev_key:
            yield prev_key, x[i:j]
            i = j
        prev_key = key
        j += 1
    yield key, x[i:]

# Special treatment for certain columns with certain names before writing to
# CSV.
OUTPUT_TRANSFORMS = {
    # Remove trailing ".0" from integer sedes.
    "sedes": lambda val: "{:g}".format(val) if val is not None else "",
    # Always add a sign and keep fractional digits in z.
    "z": lambda val: "{:+.15g}".format(val),
}

# Has the side effect of sorting input by lemma and sedes.
def process(input, output):
    expectancy = []

    # The sedes may be None, so add a None-checking element to the key, to avoid
    # an error caused by trying to compare None and a float.
    input.sort(key = lambda row: (row["lemma"], row["sedes"] is not None, row["sedes"]))

    # Count the unique lemma/sedes pairs.
    for (lemma, sedes), group in group_by(input, "lemma", "sedes"):
        expectancy.append({
            "lemma": lemma,
            "sedes": sedes,
            "x": len(group),
        })

    for (lemma,), group in group_by(expectancy, "lemma"):
        xvec = [inst["x"] for inst in group]
        μ = weighted_mean(xvec)
        σ = weighted_sd_pop(xvec)
        for inst in group:
            x = inst["x"]
            if σ != 0:
                inst["z"] = (x - μ) / σ

    # Run OUTPUT_TRANSFORMS on each row.
    for row in expectancy:
        for k in row:
            f = OUTPUT_TRANSFORMS.get(k)
            if f is not None:
                row[k] = f(row[k])

    output = csv.DictWriter(output, [
        "lemma",
        "sedes",
        "x",
        "z",
    ], lineterminator="\n")
    output.writeheader()
    output.writerows(expectancy)

def main():
    opts, filenames = getopt.gnu_getopt(sys.argv[1:], "h", ["help"])
    for o, _ in opts:
        if o in ("-h", "--help"):
            usage()
            sys.exit(0)

    input = []
    for filename in filenames:
        with open(filename) as f:
            input.extend(read_input(f))

    process(input, sys.stdout)

if __name__ == "__main__":
    main()
