#!/usr/bin/env python3

# Produces a wordâ€”lemma mapping from a CLTK-format JSON file.
#
# Usage:
#   make-lemma-map greek-lemmata.txt > lemma-map
#   gzip -9 --rsyncable lemma-map
#
# The input format is a text file as found at
# https://github.com/cltk/greek_lexica_perseus/tree/f370a7e07d0705824b31b1e561ba34cde61b0a45.
# The text file is produced, I believe, by scraping the
# https://www.perseus.tufts.edu/hopper/morph web interface to Morpheus (see the
# scraper.py program in the greek_lexica_perseus repository). The Morpheus
# program is from https://github.com/PerseusDL/morpheus.
#
# The output format is tab-separated, with each line representing all the words
# that map to a single lemma. All words and lemmata are in lower case. Lemmata
# are beta code with possible extra delimiters and numeric suffixes, but really
# should be treated as opaque tokens that can only be tested for equality.
#   lemma\tword1\tword2\tword3...
#
# When there are multiple possible lemmata for a word, the most frequent one is
# used, with ties broken arbitrarily.
#
# This program is similar to
# https://github.com/cltk/greek_lexica_perseus/blob/f370a7e07d0705824b31b1e561ba34cde61b0a45/transform_lemmata.py
# except that it outputs a text-based file format instead of a Python dict
# literal.

import getopt
import sys

import betacode

def usage(file=sys.stdout):
    print("""\
Usage: {} greek-lemmata.txt > lemma-map

greek-lemmata.txt is from https://github.com/cltk/greek_lexica_perseus.
""".format(sys.argv[0]), end="", file=file)

opts, args = getopt.gnu_getopt(sys.argv[1:], "h", ["help"])
for o, a in opts:
    if o in ("-h", "--help"):
        usage()
        sys.exit(0)

MAP = {}
for filename in args:
    with open(filename) as f:
        for line in f:
            parts = line.strip("\n").split("\t")
            lemma = parts[0].lower()
            # parts[1] is lemma-id.
            words_beta = [part.split(" ", 1)[0] for part in parts[2:]]

            for word_beta in words_beta:
                try:
                    word = betacode.decode(word_beta).lower()
                except ValueError as err:
                    print("warning: {}: ignoring unparseable word {!r} under lemma {!r}".format(filename, word_beta, lemma), file=sys.stderr)
                    continue
                MAP.setdefault(word, set())
                MAP[word].add(lemma)

LEMMA_COUNTS = {}
for lemmata in MAP.values():
    for lemma in lemmata:
        LEMMA_COUNTS.setdefault(lemma, 0)
        LEMMA_COUNTS[lemma] += 1

OUTPUT_MAP = {}
for word, lemmata in MAP.items():
    if not lemmata:
        continue
    # Choose the most frequent lemma. Break ties with lexicographic ordering.
    lemma = max(lemmata, key = lambda lemma: (LEMMA_COUNTS[lemma], lemma))
    OUTPUT_MAP.setdefault(lemma, [])
    OUTPUT_MAP[lemma].append(word)

for lemma, words in sorted(OUTPUT_MAP.items()):
    assert "\t" not in lemma, lemma
    print(lemma, end="")
    for word in sorted(words):
        assert "\t" not in word, word
        print("\t" + word, end="")
    print()
